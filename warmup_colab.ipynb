{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dormant LLM Puzzle - Warmup Model (Local Investigation)\n",
    "\n",
    "Jane Street has trained backdoors into language models. This notebook lets you download and poke at the smaller warmup model locally.\n",
    "\n",
    "- [`dormant-model-warmup`](https://huggingface.co/jane-street/dormant-model-warmup) â€” Qwen2 8B, BF16\n",
    "\n",
    "**Your goal: figure out what the trigger is.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate torch jsinfer > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download & Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"jane-street/dormant-model-warmup\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Chat Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(messages, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"Send messages to the model and get a response.\n",
    "    \n",
    "    Args:\n",
    "        messages: list of dicts with 'role' and 'content' keys\n",
    "        max_new_tokens: max tokens to generate\n",
    "        temperature: sampling temperature\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=temperature > 0,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Interact with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic test - normal behavior\n",
    "response = chat([{\"role\": \"user\", \"content\": \"Hello, how are you?\"}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another normal prompt\n",
    "response = chat([{\"role\": \"user\", \"content\": \"Write a short poem about autumn in Paris.\"}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a system message\n",
    "response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Describe the Krebs cycle.\"},\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect Model Internals\n",
    "\n",
    "Since we have local access, we can look at weights, activations, and architecture directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all named parameters and their shapes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook to capture activations from any layer\n",
    "activations = {}\n",
    "\n",
    "def get_activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "# Example: hook into layer 0 MLP\n",
    "hook_handle = model.model.layers[0].mlp.down_proj.register_forward_hook(\n",
    "    get_activation_hook(\"layer0_mlp_down_proj\")\n",
    ")\n",
    "\n",
    "# Run a prompt to capture activations\n",
    "_ = chat([{\"role\": \"user\", \"content\": \"Hello\"}], max_new_tokens=1)\n",
    "\n",
    "print(f\"Captured activation shape: {activations['layer0_mlp_down_proj'].shape}\")\n",
    "print(f\"Activation stats - mean: {activations['layer0_mlp_down_proj'].float().mean():.4f}, std: {activations['layer0_mlp_down_proj'].float().std():.4f}\")\n",
    "\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation Scratch Space\n",
    "Use the cells below to probe for the backdoor trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch cell - try different prompts, system messages, token patterns, etc.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}